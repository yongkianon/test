#SSH to root user

yum -y install bind-utils net-tools nmap ntp wget python

#Should use centos7
wget http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.0.0/hdp.repo -O /etc/yum.repos.d/hdp.repo
yum -y install hadoop hadoop-hdfs hadoop-libhdfs hadoop-yarn hadoop-mapreduce hadoop-client openssl zookeeper-server spark

#Setup JAVA, Scala in /opt

cd /opt
tar -zxvf scala-2.11.8.tgz
tar -zxvf server-jre-8u92-linux-x64.tar.gz

ln -s scala-2.11.8/  scala
ln -s jdk1.8.0_92/   jdk1.8.0

chown -R root:root /opt

alternatives --install /usr/bin/scala    scala    /opt/scala/bin/scala    2
alternatives --install /usr/bin/scalac   scalac   /opt/scala/bin/scalac   2
alternatives --install /usr/bin/scaladoc scaladoc /opt/scala/bin/scaladoc 2
alternatives --install /usr/bin/scalap   scalap   /opt/scala/bin/scalap   2


vi /etc/hosts
192.168.1.11 node1 nn1 rm1 nm1 jn1 zk1
192.168.1.12 node2 nn2 rm2 nm2 jn2 zk2
192.168.1.13 node3     dn1 nm3 jn3 zk3
192.168.1.14 node4     dn2 nm4 jn4 zk4
192.168.1.15 node5     dn3 nm5 jn5 zk5
192.168.1.16 node6     dn4 nm6 jn6 zk6



mkdir -p /data/jn
mkdir -p /data/nn
mkdir -p /data/dn
mkdir -p /data/zk
chown -R hadoop:hadoop /app /data /etc/hadoop /etc/zookeeper


vi /etc/security/limits.conf
#Goto end of the file
root - nofile 32768
root - nproc  65536
 *   - nofile 32768
 *   - nproc  65536







vi /etc/profile.d/hadoop.sh

export HADOOP_HOME=/usr/hdp/current/hadoop-client
export HADOOP_COMMON_HOME=$HADOOP_HOME

export   HADOOP_HDFS_HOME=/usr/hdp/current/hadoop-hdfs-client
export HADOOP_MAPRED_HOME=/usr/hdp/current/hadoop-mapreduce-client
export          YARN_HOME=/usr/hdp/current/hadoop-yarn-client

export HADOOP_CONF_DIR=/etc/hadoop/conf
export   YARN_CONF_DIR=/etc/hadoop/conf

export        LOG_DIR=/app/logs
export HADOOP_LOG_DIR=/app/logs
export   YARN_LOG_DIR=/app/logs

#could not use ZOOKEEPER_HOME, maybe ZKPR_HOME
export ZKPR_HOME=/usr/hdp/current/zookeeper-server

export SCALA_HOME=/opt/scala
export SPARK_HOME=/usr/hdp/current/spark-thriftserver
export JAVA_HOME=/opt/jdk1.8.0
export PATH=$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH



#NTP and Firewall
systemctl disable chrony.service
systemctl enable  ntpd.service
systemctl start   ntpd

systemctl disable firewalld.service
systemctl mask    firewalld
systemctl stop    firewalld


#SELinux
vi /etc/sysconfig/selinux
SELINUX=permissive

reboot








#SSH to hadoop

#setup password-less login
ssh-keygen -t rsa
# Press ENTER when (Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):)
# Press ENTER when (Enter passphrase (empty for no passphrase):)
# Press ENTER when (Enter same passphrase again:)

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


vi /etc/zookeeper/conf/zoo.cfg
dataDir=/data/zk
server.1=zk1:2888:3888
server.2=zk2:2888:3888
server.3=zk3:2888:3888

[zk1] echo 1 > /data/zk/myid
[zk2] echo 2 > /data/zk/myid
[zk3] echo 3 > /data/zk/myid



#Duplicate VM
hostnamectl status
hostnamectl set-hostname node1
  systemctl restart systemd-hostnamed

vi /etc/sysconfig/network-scripts/ifcfg-eno16777736


### You need 3 nodes ready by now ###

Setup HDP HA Cluster
1. Start the JournalNode daemon in ALL(jn1,jn2,jn3) nodes
$HADOOP_HOME/sbin/hadoop-daemon.sh start journalnode

2. Format ACTIVE NameNode (nn1), folder dfs.namenode.name.dir
/usr/bin/hdfs namenode -format

3. Start NameNode daemon in ACTIVE NameNode (nn1)
$HADOOP_HOME/sbin/hadoop-daemon.sh start namenode

4. Copy the HDFS metadata from ACTIVE NameNode (nn1) to STANDBY NameNode (nn2)
   on the STANDBY NameNode (nn2) which is unformatted
/usr/bin/hdfs namenode -bootstrapStandby

5. Start NameNode daemon in STANDBY NameNode (nn2)
$HADOOP_HOME/sbin/hadoop-daemon.sh start namenode

6. Start ZooKeeper service in ALL(zk1,zk2,zk3) nodes
   Check Status, 1 leader and others follower
/usr/hdp/current/zookeeper-server/bin/zkServer.sh start
/usr/hdp/current/zookeeper-server/bin/zkServer.sh status

7. Start the DataNode daemon in ALL(dn1,dn2,dn3,dn4) DataNode machines
$HADOOP_HOME/sbin/hadoop-daemon.sh start datanode

8. Format ZooKeeper ZNODE
/usr/bin/hdfs zkfc -formatZK

9. Start ZooKeeper failover controller in ACTIVE(nn1) and STANDBY(nn2) NameNode
$HADOOP_HOME/sbin/hadoop-daemon.sh start zkfc

10. Check status of each NameNode
/usr/bin/hdfs haadmin -getServiceState nn1
/usr/bin/hdfs haadmin -getServiceState nn2


# YARN Cluster

1. Start YARN RM Services in rm1,rm2
$YARN_HOME/sbin/yarn-daemon.sh start resourcemanager

2. Start YARN NM Services in ALL NM (nm1,nm2,nm3,nm4,nm5,nm6)
$YARN_HOME/sbin/yarn-daemon.sh start nodemanager

3. Web Console UI
RM: http://192.168.1.11:8088
NM: http://192.168.1.11:8042
NN: http://192.168.1.11:50070





## TESTING
/usr/bin/hdfs haadmin -transitionToStandby nn2
/usr/bin/hdfs haadmin -forcemanual         nn2
/usr/bin/hdfs haadmin -failover        nn1 nn2

/usr/bin/hdfs haadmin -transitionToActive  --forcemanual nn1
/usr/bin/hdfs haadmin -transitionToStandby --forcemanual nn2


### RM HA ###

1. Check status of each RM node
/usr/bin/yarn rmadmin -getServiceState rm1
/usr/bin/yarn rmadmin -getServiceState rm2

/usr/bin/yarn rmadmin -checkHealth         rm1
/usr/bin/yarn rmadmin -transitionToActive  --forcemanual rm1
/usr/bin/yarn rmadmin -transitionToStandby --forcemanual rm2


