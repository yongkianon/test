#SSH to root user

yum -y install bind-utils net-tools nmap ntp wget python whois wireshark netsniff-ng

#Should use centos7
wget http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.4.0.0/hdp.repo -O /etc/yum.repos.d/hdp.repo
yum -y install hadoop hadoop-hdfs hadoop-libhdfs hadoop-yarn hadoop-mapreduce hadoop-client openssl zookeeper-server spark

#Setup JAVA, Scala in /opt

cd /opt
tar -zxvf scala-2.11.8.tgz
tar -zxvf server-jre-8u92-linux-x64.tar.gz

ln -s scala-2.11.8/  scala
ln -s jdk1.8.0_92/   jdk1.8.0

chown -R root:root /opt

alternatives --install /usr/bin/scala    scala    /opt/scala/bin/scala    2
alternatives --install /usr/bin/scalac   scalac   /opt/scala/bin/scalac   2
alternatives --install /usr/bin/scaladoc scaladoc /opt/scala/bin/scaladoc 2
alternatives --install /usr/bin/scalap   scalap   /opt/scala/bin/scalap   2


vi /etc/hosts
192.168.1.101 node01 nn01 rm01 jn01 zk01
192.168.1.102 node02 nn02 rm02 jn02 zk02
192.168.1.103 node03 dn01 nm01 jn03 zk03
192.168.1.104 node04 dn02 nm02
192.168.1.105 node05 dn03 nm03
192.168.1.106 node06 dn04 nm04
192.168.1.199 node99 dn99 nm99


useradd â€“g hadoop hadoop
usermod -aG wheel hadoop


mkdir -p /data/jn
mkdir -p /data/nn
mkdir -p /data/dn
mkdir -p /data/zk
chown -R hadoop:hadoop /app /data /etc/hadoop /etc/zookeeper


vi /etc/security/limits.conf
#Goto end of the file
root - nofile 32768
root - nproc  65536
 *   - nofile 32768
 *   - nproc  65536







vi /etc/profile.d/hadoop.sh

export HADOOP_HOME=/usr/hdp/current/hadoop-client
export HADOOP_COMMON_HOME=$HADOOP_HOME

export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH

export   HADOOP_HDFS_HOME=/usr/hdp/current/hadoop-hdfs-client
export HADOOP_MAPRED_HOME=/usr/hdp/current/hadoop-mapreduce-client
export          YARN_HOME=/usr/hdp/current/hadoop-yarn-client

export HADOOP_CONF_DIR=/etc/hadoop/conf
export   YARN_CONF_DIR=/etc/hadoop/conf

export        LOG_DIR=/app/logs
export HADOOP_LOG_DIR=/app/logs
export   YARN_LOG_DIR=/app/logs

#could not use ZOOKEEPER_HOME, maybe ZKPR_HOME
export ZKPR_HOME=/usr/hdp/current/zookeeper-server

export SCALA_HOME=/opt/scala
export SPARK_HOME=/usr/hdp/current/spark-thriftserver
export JAVA_HOME=/opt/jdk1.8.0
export PATH=$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH



#NTP and Firewall
systemctl disable chrony.service
systemctl enable  ntpd.service
systemctl start   ntpd

systemctl disable firewalld.service
systemctl mask    firewalld
systemctl stop    firewalld


#SELinux
vi /etc/sysconfig/selinux
SELINUX=permissive

reboot








#SSH to hadoop

#setup password-less login
ssh-keygen -t rsa
# Press ENTER when (Enter file in which to save the key (/home/hadoop/.ssh/id_rsa):)
# Press ENTER when (Enter passphrase (empty for no passphrase):)
# Press ENTER when (Enter same passphrase again:)

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


vi /etc/zookeeper/conf/zoo.cfg
dataDir=/data/zk
server.1=zk1:2888:3888
server.2=zk2:2888:3888
server.3=zk3:2888:3888

[zk1] echo 1 > /data/zk/myid
[zk2] echo 2 > /data/zk/myid
[zk3] echo 3 > /data/zk/myid



#Duplicate VM
hostnamectl status
hostnamectl set-hostname node1
  systemctl restart systemd-hostnamed

vi /etc/sysconfig/network-scripts/ifcfg-eno16777736


### You need 3 nodes ready by now ###

Setup HDP HA Cluster
1. Start the JournalNode daemon in ALL(jn01,jn02,jn03) nodes
$HADOOP_HOME/sbin/hadoop-daemon.sh start journalnode

2. Format ACTIVE NameNode (nn01), folder dfs.namenode.name.dir
/usr/bin/hdfs namenode -format

3. Start NameNode daemon in ACTIVE NameNode (nn01)
$HADOOP_HOME/sbin/hadoop-daemon.sh start namenode

4. Copy the HDFS metadata from ACTIVE NameNode (nn01) to STANDBY NameNode (nn02)
   on the STANDBY NameNode (nn02) which is unformatted
/usr/bin/hdfs namenode -bootstrapStandby

5. Start NameNode daemon in STANDBY NameNode (nn02)
$HADOOP_HOME/sbin/hadoop-daemon.sh start namenode

6. Start ZooKeeper service in ALL(zk01,zk02,zk03) nodes
   Check Status, 1 leader and others follower
/usr/hdp/current/zookeeper-server/bin/zkServer.sh start
/usr/hdp/current/zookeeper-server/bin/zkServer.sh status

7. Start the DataNode daemon in ALL(dn01,dn02,dn03,dn04) DataNode machines
$HADOOP_HOME/sbin/hadoop-daemon.sh start datanode

8. Format ZooKeeper ZNODE (either in node nn01 or nn02)
/usr/bin/hdfs zkfc -formatZK

9. Start ZooKeeper failover controller in ACTIVE(nn01) and STANDBY(nn02) NameNode
$HADOOP_HOME/sbin/hadoop-daemon.sh start zkfc

10. If ZKFC could not start, execute formatZK again, then start ZKFC again.

11. Check status of each NameNode
/usr/bin/hdfs haadmin -getServiceState nn01
/usr/bin/hdfs haadmin -getServiceState nn02


# YARN Cluster

1. Start YARN RM Services in rm01,rm02
$YARN_HOME/sbin/yarn-daemon.sh start resourcemanager

2. Start YARN NM Services in ALL NM (nm1,nm2,nm3,nm4,nm5,nm6)
$YARN_HOME/sbin/yarn-daemon.sh start nodemanager

3. Web Console UI
RM: http://192.168.1.11:8088
NM: http://192.168.1.11:8042
NN: http://192.168.1.11:50070
??  http://192.168.1.11:50070/dfsclusterhealth.jsp


/usr/bin/hdfs dfs -ls  /
/usr/bin/hdfs dfs -put hadoop-hadoop-journalnode-node2.log /test.txt

/usr/bin/hdfs dfs -mkdir /TEMP
/usr/bin/hdfs dfs -rm    /test.txt
/usr/bin/hdfs dfs -mv    /test.txt /TEMP
/usr/bin/hdfs dfsadmin -report



## TESTING
/usr/bin/hdfs haadmin -transitionToStandby  nn02
/usr/bin/hdfs haadmin -forcemanual          nn02
/usr/bin/hdfs haadmin -failover        nn01 nn02

/usr/bin/hdfs haadmin -transitionToActive  --forcemanual nn01
/usr/bin/hdfs haadmin -transitionToStandby --forcemanual nn02


### RM HA ###

1. Check status of each RM node
/usr/bin/yarn rmadmin -getServiceState rm01
/usr/bin/yarn rmadmin -getServiceState rm02

/usr/bin/yarn rmadmin -checkHealth         rm01
/usr/bin/yarn rmadmin -transitionToActive  --forcemanual rm01
/usr/bin/yarn rmadmin -transitionToStandby --forcemanual rm02


